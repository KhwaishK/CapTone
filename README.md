# ğŸ–¼ï¸ AI Captioning & Rephrasing Tool

This project generates captions for images using **BLIP (Bootstrapped Language Image Pretraining)** and then rephrases them into different tones using **LLaMA 3.1**, with hashtag generation powered by **KeyBERT**.  
It also includes model evaluation on a mini COCO dataset subset, using **BLEU scores** for caption quality assessment.

---

## ğŸš€ Live Demo

ğŸ¯ Try the live app here:  
ğŸ‘‰ [**Hugging Face Demo**](https://huggingface.co/spaces/KhwaishK/CapTone)

---

## ğŸ“¸ Features

- ğŸ§  **Automatic Image Captioning** using Salesforce BLIP model  
- ğŸ·ï¸ **Hashtag Generation** using KeyBERT keyword extraction  
- âœï¸ **Tone Rephrasing** with LLaMA 3.1 (Funny, Professional, Poetic, and Marketing/Ad tones)  
- ğŸ§ª **Model Evaluation** on a mini COCO dataset with BLEU scores  
- ğŸ–¥ï¸ **Streamlit Web App** for easy user interaction  

---

## ğŸ§© Project Structure
```graphql
ğŸ“¦ AI-Captioning-Rephrasing-Tool
â”‚
â”œâ”€â”€ app.py                     # Streamlit-based user interface
â”œâ”€â”€ pipeline.py                # Core pipeline: captioning, hashtags, rephrasing
â”œâ”€â”€ prepare_mini_dataset.py    # Script to create a mini COCO validation dataset
â”œâ”€â”€ model_testing.py           # Evaluates BLIP model performance on mini dataset (BLEU)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ annotations/           # COCO-style annotation JSONs
â”‚   â”œâ”€â”€ mini_val2014/          # Subset of COCO validation images
â”‚   â””â”€â”€ samples/               # Sample images for testing
â”‚
â”œâ”€â”€ params.yaml                # Configuration for model paths, UI options, and parameters
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation & Setup
### 1ï¸âƒ£ Clone the repository 
```bash
git clone https://github.com/KhwaishK/CapTone.git
cd AI-Captioning-Rephrasing-Tool
```

### 2ï¸âƒ£ Create a virtual environment
```bash
python -m venv captionenv
captionenv\Scripts\activate   # (Windows)
# OR
source captionenv/bin/activate  # (Mac/Linux)
```

### 3ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```
### â–¶ï¸ Running the App Locally
```bash
streamlit run app.py
```
Then open the provided local URL (usually http://localhost:8501) in your browser.

---

## âš¡ How It Works
-Image Upload â†’ User uploads an image (or uses default sample).

-Caption Generation â†’ BLIP model generates a natural caption.

-Hashtag Extraction â†’ KeyBERT extracts top keywords as hashtags.

-Tone Rephrasing â†’ User selects a tone and LLaMA 3.1 rewrites the caption accordingly.
